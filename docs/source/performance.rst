==========================
Performance tuning 
==========================

One of the main goals of wormtable is to 
provide a high performance platform for processing tables, 
but while also keeping things as simple as possible.
While wormtable should be quite efficient 
in most cases, there are a few simple things to do which can 
improve performance considerably when working with very large 
tables.

-------------
Schema tuning
-------------

To get the best performance possible from wormtable, it is important that 
the rows are as small as possible. This is done by either using the smallest
possible type for a given column, or discarding the column entirely if
it is not needed. Schemas generated using ``vcf2wt`` for VCF files are 
conservative, and can often be improved on considerably by some hand tuning. By 
shaving a few tens of bytes off each row we can sometimes make the table
gigabytes smaller. Since the main thing that slows down wormtable is 
reading data off disk, the smaller the table the faster it is.


To tune the schema of a VCF file, we must first use the ``--generate`` option in 
``vcf2wt`` to generate a candidate schema::

    $ vcf2wt -g data.vcf schema.xml

The ``schema.xml`` file then contains the automatically generated (conservative) 
schema. We can then edit this schema as we see fit, chaning and deleting columns 
as we see fit. Once we're happy with the schema, we can then build the table 
using this new schema using the ``--schema`` option::

    $ vcf2wt -s schema.xml data.vcf data.wt 

There are many ways in which we can make a more compact schema. 
In the following, we'll work with a 
VCF file  from the Drosophila Genetic Reference Panel (available 
`here <ftp://ftp.hgsc.bcm.edu/DGRP/freeze2_Feb_2013/vcf_files/freeze2.vcf.gz>`_),
improving the schema from that automatically generated by ``vcf2wt``.

Consider,
for example, the following segment of the schema generated by 
``vcf2wt``:

.. code-block:: xml 
    
    <column description="Genotype" element_size="1" element_type="char" name="DGRP-021.GT" num_elements="var(1)"/>
    <column description="number of supporting reads" element_size="4" element_type="int" name="DGRP-021.SR" num_elements="1"/>
    <column description="number of opposing reads" element_size="4" element_type="int" name="DGRP-021.OR" num_elements="1"/>
    <column description="Genotype quality" element_size="4" element_type="int" name="DGRP-021.GQ" num_elements="1"/>

There are several ways in which this schema is less than optimal. Firstly, we know that 
this VCF is for diploids, and so the genotype columns (e.g ``DGRP-021.GT`` above) 
are always exactly three characters long. Yet, in this schema the number of 
elements is ``var(1)``, allowing variable sized strings to be stored in this column. Variable 
sized columns have an overhead of three bytes above the actual values stored, 
and so in this case we using twice as many bytes as we should be. To rectify this, 
we change the ``num_elements`` to ``3``:

.. code-block:: xml 

    <column description="Genotype" element_size="1" element_type="char" name="DGRP-021.GT" num_elements="3"/>

The next two columns are of the number of supporting reads and number of opposing reads
at a variant site, and these are stored using four byte integers. Therefore, these 
columns ``DGRP-021.SR`` and ``DGRP-21.OR`` can store integers in the range
-2147483647 and 2147483647
(see  :ref:`int-types-index`)
. This range is far too large. A more suitable type for 
these columns are 2 byte unsigned integers, giving a range of 
0 to 65534 (see  :ref:`uint-types-index`). The new lines look like:

.. code-block:: xml 
    
    <column description="number of supporting reads" element_size="2" element_type="uint" name="DGRP-021.SR" num_elements="1"/>
    <column description="number of opposing reads" element_size="2" element_type="uint" name="DGRP-021.OR" num_elements="1"/>

Finally, we see that the genotype quality column is also a four byte signed integer, when a 1 byte unsigned 
integer suffices to store the values in this VCF:

.. code-block:: xml 

    <column description="Genotype quality" element_size="1" element_type="int" name="DGRP-021.GQ" num_elements="1"/>

We have saved a total of 8 bytes over the default schema by making these 
changes. This hardly seems worth the effort, but is in fact quite significant 
for two reasons. Firstly, every byte save per row really does count when we 
are storing millions of rows. Secondly, there are 205 genotypes in this VCF
so we can make a saving of 1640 bytes by applying these changes to all 
of the relevant columns.

Another way in which we can save space is to delete columns that we are not interested 
in or that don't contain any information. For example, in the Drosophila VCF above,
the ``ID`` and ``QUAL`` columns contain only missing values, and the ``FILTER``
column only contains only ``PASS``. We can simply delete these columns from the 
schema, to save another 14 bytes per row.

This tweaking makes a considerable difference.
The source VCF file is 2.8GB when gzip compressed, and 15GB uncompressed. When we
use the automatic schema from ``vcf2wt`` the resulting wormtable is 23.5GB.
However, when we make the changes mentioned above, the wormtable requires only
10.2GB. 

.. warning:: It is very important that rows are less than approximately 16K. Over this 
   threshold, performance degrades considerably.

.. _performance-cache:

------------
Cache tuning
------------

It is very important to provide a large cache when creating a 
new table or index. The cache size in wormtable roughly controls 
the amount of the table that is stored in memory. This 
is one of the advanced features offered by Berkeley DB, 
and greatly reduces the amount of time required to write 
large tables.

As a rule of thumb, it 
is a good idea to set aside half of available RAM for cache 
when writing new tables.
So, for example, in a system with 16GB of RAM, a good amount of 
cache to allocate would be 8GB. This may seem like a very large 
amount of memory to dedicate to cache, but the more of the 
underlying Berkeley DB that fits into the cache the better 
performance will be as we avoid the costly process of writing 
pages to disc which may need to be read back in later.
Ideally, we would like to fit the entire DB into memory
while we are generating it, which means we only need to 
write each page to disc once. 

It is also important to allocate a large cache then creating a new 
index, although we rarely need as much as when creating a table.
Most of the time the entire index will fit comfortably in 
RAM, which makes writing the index much faster. The cache 
size specified is the *maximum* amount to use, and so 
if the index will fit into less memory than we have allocated
for cache, the remaining memory will not be used.

When reading tables, we rarely need as much cache as when 
we are writing them  The amount of cache to allocate 
to different indexes and to the main table is a subtle issue
and depends very much on the workload. Generally,
for a linear scan of a table very little cache is 
required. However, in situations where we are iterating 
over rows that are not in table order using an index, 
it can be helpful to have a large cache.

For further information, see the discussion on setting cache 
sizes for
`Berkeley DB <http://docs.oracle.com/cd/E17076_02/html/programmer_reference/general_am_conf.html#am_conf_cachesize>`_.


